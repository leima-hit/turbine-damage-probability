classifyTurbines <- function(trainData, trainClass
                             , testData, testClass
                             , predictorAttributes # attribute names to be used for classification
                             , tuneLength = NULL # tuneLength parameter : passed to CARET train function
                             , resamplingMethod = 'repeatedCV' # resampling process : can be 'repeatedCV' or 'adaptiveCV'
                             , preProcessStr = NULL # pre-processing function : passed to CARET train function
                             , preProcessFlag = F # pre-processing FLag
                             , parallelFlag = T # use parallel processing
                             , classifierNames = c('pls', 'svmRadial') # can be a vector of different classifiers available in CARET
                             , numCores = detectCores()/2 # number of cores to be used for parallel processing
                             , oldClassifierModels = NULL # pass an existing classifier model based on old training set
)
{
    # classifies turbines using different classifier models
    # can also be used for updating pre-existing classifier models

    seedValue <- 666
    set.seed(seedValue)

    # set up parallel processing
    if (parallelFlag && numCores != 0) cluster <- makeCluster(numCores)

    # performance metric
    metric = 'ROC'

    # set tuneLength if not passed
    if (is.null(tuneLength))
    {
        tuneLength <- length(names(trainData))
    }


    indivClassifierFlag <- F # flag for using only individual classifiers for updating pre-existing classifiers
    if (sum(!(grepl('strike', names(trainData)))) < 2)
    {
        # if training data contains prediction probabilities from multiple classifiers then use all such columns
        indivClassifierFlag <- T
    }



    # # check for zero variance variables and remove those
    # nearZeroVarAttributes = nearZeroVar(trainData[, predictorAttributes], saveMetrics = T)
    # predictorAttributes = predictorAttributes[nearZeroVarAttributes$percentUnique >= 20]


    trainData <- trainData[predictorAttributes]
    testData <- testData[predictorAttributes]

    # create temporary variables to store original data
    tmpTrainData <- trainData
    tmpTestData <- testData


    #Pre-Compute CV folds so we can use the same ones for all models
    set.seed(seedValue)
    CV_Folds <- createMultiFolds(trainClass, k = 10, times = 5)


    # set resampling technique
    if (resamplingMethod == 'repeatedCV')
    {
        ctrlParams = trainControl(method = "repeatedcv"
                                  # , number = 10, repeats = 5
                                  , index = CV_Folds
                                  , search = 'random'# selectionFunction = "oneSE",
                                  , classProbs = T, summaryFunction = twoClassSummary
                                  , allowParallel = parallelFlag, verboseIter = F
                                  , savePredictions = T
                                  # , search = 'random'
                                  ## Adaptive resampling information:
                                  # adaptive = list(min = 5, alpha = 0.05, method = "gls", complete = TRUE)
        )
    } else if (resamplingMethod == 'adaptiveCV') {
        ctrlParams = trainControl(method = "adaptive_cv"
                                  # , number = 10, repeats = 5
                                  , index = CV_Folds
                                  , classProbs = T, summaryFunction = twoClassSummary
                                  , allowParallel = parallelFlag, verboseIter = F
                                  , savePredictions = T
                                  # , search = 'random'
                                  ## Adaptive resampling information:
                                  , adaptive = list(min = 5, alpha = 0.05, method = "gls", complete = T)
        )
    }


    # start parallel processing
    if (parallelFlag && numCores != 0) registerDoParallel(cluster)


    # create variable to hold all the trained models
    classifierModels <- vector("list", length(classifierNames))
    iModel <- 1

    # perform classification
    for (iName in classifierNames)
    {
        print(iName)
        set.seed(seedValue)

        preProcessStrTmp <- NULL

        if (!is.null(oldClassifierModels[[iName]]) && indivClassifierFlag)
        {
            # if updating pre-existing classifiers with individual classifier models then modify training set by
            # appending the prediction probabilities generated by the old classifier model for the current training set

            predProbs <- predict(oldClassifierModels[[iName]], tmpTrainData, type = 'prob')[,2]
            trainData <- cbind(tmpTrainData, predProbs)

            # do the same for test data as well
            predProbs <- predict(oldClassifierModels[[iName]], tmpTestData, type = 'prob')[,2]
            testData <- cbind(tmpTestData, predProbs)

            idx <- which(names(trainData) == 'predProbs')
            names(trainData)[idx] <- iName
            names(testData)[idx] <- iName
        }

        if (iName == 'pls')
        {

            ## PLS
            if (preProcessFlag)
            {
                preProcessStrTmp <- ifelse(is.null(preProcessStr), 'BoxCox', preProcessStr)
            }

            plsGrid = data.frame(ncomp = seq(1, length(predictorAttributes), 1))
            modelFit <- train(trainData, trainClass,
                              method = "pls", trControl = ctrlParams, metric = metric, preProcess = preProcessStrTmp,
                              # tuneLength = tuneLength)
                              tuneGrid = plsGrid)

        } else if (iName == 'rda') {

            ## RDA
            rdaGrid = expand.grid(gamma = (0:4)/4, lambda = (0:4)/4)
            modelFit <- train(trainData, trainClass,
                              method = "rda", preProcess = preProcessStr,
                              tuneGrid = rdaGrid, trControl = ctrlParams, metric = metric)

        } else if (iName == 'rf') {

            ## Random Forest
            rfGrid <- data.frame(mtry = seq(1, (length(predictorAttributes)), 1))

            if (preProcessFlag)
            {
                preProcessStrTmp <- ifelse(is.null(preProcessStr), 'BoxCox', preProcessStr)
            }

            modelFit = train(trainData, trainClass,
                             method = "rf", trControl = ctrlParams, metric = metric,
                             preProcess = preProcessStrTmp,
                             # tuneGrid = rfGrid)
            tuneLength = tuneLength)

        } else if (iName == 'parRF') {

            ## Random Forest
            parRFGrid <- data.frame(mtry = seq(1, sqrt(length(predictorAttributes)), 1))
            modelFit = train(trainData, trainClass,
                             method = "parRF", trControl = ctrlParams, metric = metric, preProcess = preProcessStr,
                             tuneGrid = parRFGrid)
            # tuneLength = tuneLength)

        } else if (iName == 'gbm') {

            ## Boosted Tree
            gbmGrid <- expand.grid(interaction.depth = seq(1,10,by=2), # look at tree depths from 1 to 7
                                   n.trees=seq(10,100,by=5),	        # let iterations go from 10 to 100
                                   shrinkage=c(0.01, 0.1, 1, 10),		        # Try 2 values of learning rate parameter
                                   n.minobsinnode = 2)
            modelFit = train(trainData, trainClass,
                             method = "gbm",
                             # distribution = 'gaussian',
                             trControl = ctrlParams, metric = metric, verbose = F, preProcess = preProcessStr,
                             # tuneGrid = gbmGrid)
                             tuneLength = tuneLength)
            # trellis.par.set(caretTheme())
            # ggplot(gbmFit, metric = 'ROC')

        } else if (iName == 'svmPoly') {

            ## SVM Polynomial Kernel
            svmGrid <- expand.grid(C = 10^(-3:3),
                                   scale = 1,
                                   degree = 1:3)
            modelFit = train(trainData, trainClass,
                             method = "svmPoly", trControl = ctrlParams, metric = metric, preProcess = preProcessStr,
                             tuneLength = tuneLength)
                             # tuneGrid = svmGrid)


        } else if (iName == 'svmRadialCost') {

            ## SVM Radial Kernel
            # svmGrid <- data.frame(C = 10^(-3:3))
            svmGrid <- data.frame(C = seq(0.05, 0.15, 0.01))
            modelFit = train(trainData, trainClass,
                             method = "svmRadialCost", trControl = ctrlParams, metric = metric, preProcess = preProcessStr, # scaled = F,
                             tuneLength = tuneLength)
                             # tuneGrid = svmGrid)

        } else if (iName == 'svmRadial') {

            ## SVM Radial Kernel
            modelFit = train(trainData, trainClass,
                             method = "svmRadial", trControl = ctrlParams, metric = metric,
                             preProcess = preProcessStr,
                             tuneLength = tuneLength)
            # does not depend on pre-processing
            # tuneGrid = svmGrid)

        } else if (iName == 'svmLinear') {

            ## SVM Linear Kernel
            svmGrid <- data.frame(C = 10^(-3:3))
            # svmGrid <- data.frame(C = seq(0.05, 0.15, 0.01))
            modelFit = train(trainData, trainClass,
                             method = "svmLinear", trControl = ctrlParams, metric = metric, preProcess = preProcessStr,
                             tuneLength = tuneLength)
                             # tuneGrid = svmGrid)

        } else if (iName == 'svmRadialWeights') {

            ## SVM Linear Kernel
            svmGrid <- expand.grid(C = 2^(-6:1), sigma = seq(0.1, 1, 0.1), Weight = 2^(-1:5))
            if (preProcessFlag)
            {
                preProcessStrTmp <- ifelse(is.null(preProcessStr), c('BoxCox'), preProcessStr)
            }
                        # svmGrid <- data.frame(C = seq(0.05, 0.15, 0.01))
            modelFit = train(trainData, trainClass,
                             method = "svmRadialWeights", trControl = ctrlParams, metric = metric,
                             preProcess = preProcessStrTmp,
                             tuneLength = tuneLength)
            # tuneGrid = svmGrid)

        } else if (iName == 'avNNet') {

            numNetworks <- 2 # 5 different NN is fit and averaged
            # ctrlParams$seeds <- runif(numNetworks)

            if (preProcessFlag)
            {
                preProcessStrTmp <- ifelse(is.null(preProcessStr), c('center', 'scale'), preProcessStr)
            }

            nnetGrid <- expand.grid(size = seq(12, 14, 1), decay = seq(0.005, 0.05, 0.0025), bag = F)
            modelFit = train(trainData, trainClass,
                             method = "avNNet", trControl = ctrlParams, metric = metric,
                             preProcess = preProcessStrTmp, # scaled = F,
                             repeats = numNetworks,
                             linout = T,
                             maxit = 100,
                             tuneLength = tuneLength)
                             # tuneGrid = nnetGrid)
        } else if (iName == 'nnet') {

            ## general classifier
            if (preProcessFlag)
            {
                preProcessStrTmp <- ifelse(is.null(preProcessStr), c('nzv', 'range'), preProcessStr)
            }

            modelFit <- train(trainData, trainClass,
                              method = 'nnet', trControl = ctrlParams, metric = metric,
                              preProcess = preProcessStrTmp,
                              maxit = 100, trace = F, verbose = F,
                              tuneLength = tuneLength)

        } else {

            ## general classifier :  if want to use any specific classifier with specific tuneLength or tuneGrid parameters
            # then add code as shown above for that one
            modelFit <- train(trainData, trainClass,
                             method = iName, trControl = ctrlParams, metric = metric, preProcess = preProcessStr,
                            # maxit = 500, trace = F, verbose = F,
                             tuneLength = tuneLength)

        }

        # modelFit$preProcValues <- preProcValues
        classifierModels[[iModel]] <- modelFit
        iModel <- iModel + 1
    }

    names(classifierModels) <- classifierNames

    # stop parallel processing
    if (parallelFlag && numCores != 0) on.exit(stopCluster(cluster))

    print(names(trainData))
    return(classifierModels)

}